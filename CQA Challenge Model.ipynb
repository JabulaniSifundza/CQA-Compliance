{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d749835b-504f-401a-8f29-065045f89e10",
   "metadata": {},
   "outputs": [],
   "source": [
    "from yahooquery import Ticker\n",
    "import pandas_datareader as pdr\n",
    "import datetime\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import warnings\n",
    "import seaborn as sns\n",
    "from sklearn.linear_model import LinearRegression\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats\n",
    "import json\n",
    "from IPython.display import FileLink\n",
    "import os\n",
    "from scipy.optimize import minimize\n",
    "from math import sqrt\n",
    "import statsmodels.api as sm\n",
    "from arch import arch_model\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.metrics import mean_squared_error\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "934696d9-80f8-4817-87c7-56095e4ad1a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = 'Nov 14 Data Update.xlsx'\n",
    "df_cleaned = pd.read_excel(file_path, sheet_name='Sheet1', skiprows=2)\n",
    "df_cleaned['ticker_symbol'] = df_cleaned['Ticker'].str.replace(' US Equity', '')\n",
    "df_cleaned = df_cleaned.drop(index=0)\n",
    "df_cleaned = df_cleaned.reset_index(drop=True)\n",
    "df_cleaned = df_cleaned.drop(df_cleaned.index[-2:])\n",
    "# df_filtered = df_cleaned[['Ticker', 'Short Name', 'GICS Sector', 'ROE LF', 'Quarterly Momentum', 'P/CF']].dropna()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d2e2410",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the 10-year Treasury yield as a proxy for the risk-free rate\n",
    "start_date = datetime.datetime(2023, 1, 1)\n",
    "end_date = datetime.datetime.now()\n",
    "\n",
    "df = pdr.DataReader('DGS10', 'fred', start_date, end_date)\n",
    "risk_free_rate = df['DGS10'].iloc[-1] / 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe3239e9-d4c1-4ab4-bf4f-7aba572a115b",
   "metadata": {},
   "outputs": [],
   "source": [
    "list_of_symbols = df_cleaned['ticker_symbol'].tolist()\n",
    "def iterate_revenue_and_net(list_of_syms):\n",
    "    arr = []\n",
    "    for symbol in list_of_syms:\n",
    "        aapl = Ticker(symbol)\n",
    "        types = ['TotalRevenue', 'NetIncome']\n",
    "        data = aapl.get_financial_data(types, trailing=False)\n",
    "        arr.append(data)\n",
    "    return arr\n",
    "\n",
    "\n",
    "test = iterate_revenue_and_net(list_of_symbols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a85c143-ea2a-411e-badb-38a9d3896c06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter out non-DataFrame items\n",
    "test = [df for df in test if isinstance(df, pd.DataFrame)]\n",
    "\n",
    "# Now concatenate\n",
    "combined_df = pd.concat(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2ed5113-6ac7-4f61-8d11-337d50210081",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df['Profit Margin'] = combined_df['NetIncome'] / combined_df['TotalRevenue']\n",
    "combined_df['Profit Margin Growth'] = combined_df.groupby('symbol')['Profit Margin'].pct_change()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e65166c3-eddb-4c28-8bcf-695a289fff70",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df = combined_df.dropna(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fb6f944-364b-4d0d-92f9-0013a7e6d300",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_ticker_and_update_df(ticker_symbol, df):\n",
    "    # Fetch stock data using yahooquery\n",
    "    stock = Ticker(ticker_symbol)\n",
    "    history = stock.history(period='2y')\n",
    "    \n",
    "    # Ensure history data exists\n",
    "    if history.empty:\n",
    "        print(f\"No data found for {ticker_symbol}\")\n",
    "        return df\n",
    "    \n",
    "    # Calculate daily log returns and add a column to the historical data\n",
    "    history['Log Return'] = np.log(history['adjclose'] / history['adjclose'].shift(1))\n",
    "    \n",
    "    # Calculate expected annual return (compounded daily return)\n",
    "    avg_daily_return = history['Log Return'].mean()\n",
    "    expected_annual_return = avg_daily_return * 252  # 252 trading days in a year\n",
    "    \n",
    "    # MACD calculation (momentum indicator)\n",
    "    short_ema = history['adjclose'].ewm(span=12, adjust=False).mean()  # 12-day EMA\n",
    "    long_ema = history['adjclose'].ewm(span=26, adjust=False).mean()   # 26-day EMA\n",
    "    macd = short_ema - long_ema                                     # MACD line\n",
    "    signal = macd.ewm(span=9, adjust=False).mean()                  # Signal line\n",
    "    macd_diff = macd - signal                                       # MACD histogram (momentum)\n",
    "    \n",
    "    # Add the momentum (MACD histogram) to the historical data\n",
    "    history['Momentum'] = macd_diff\n",
    "    \n",
    "    # Bollinger Bands Calculation\n",
    "    history['EMA'] = history['adjclose'].ewm(span=20, adjust=False).mean()\n",
    "    history['STD'] = history['adjclose'].rolling(window=20).std()\n",
    "    history['Upper Band'] = history['EMA'] + (history['STD'] * 2)\n",
    "    history['Lower Band'] = history['EMA'] - (history['STD'] * 2)\n",
    "    \n",
    "    # Relative Strength Index\n",
    "    delta = history['adjclose'].diff()\n",
    "    gain = (delta.where(delta > 0, 0)).rolling(window=14).mean()\n",
    "    loss = (-delta.where(delta < 0, 0)).rolling(window=14).mean()\n",
    "    rs = gain / loss\n",
    "    history['RSI'] = 100 - (100 / (1 + rs))\n",
    "    \n",
    "    # Get latest values for indicators\n",
    "    latest_adjclose = history['adjclose'].iloc[-1]\n",
    "    latest_upper_band = history['Upper Band'].iloc[-1]\n",
    "    latest_lower_band = history['Lower Band'].iloc[-1]\n",
    "    latest_rsi = history['RSI'].iloc[-1]\n",
    "    latest_log_return = history['Log Return'].iloc[-1]\n",
    "    latest_momentum = history['Momentum'].iloc[-1]\n",
    "    \n",
    "    # Calculate Buy/Sell signals\n",
    "    rec_consensus = df.loc[df['ticker_symbol'] == ticker_symbol, 'Rec Consensus:D-1'].values[0]\n",
    "    if latest_adjclose < latest_lower_band and latest_rsi < 30 and rec_consensus >= 4:\n",
    "        buy_sell_signal = 'Buy'\n",
    "    elif latest_adjclose > latest_upper_band and latest_rsi > 70 and rec_consensus <= 2:\n",
    "        buy_sell_signal = 'Sell'\n",
    "    else:\n",
    "        buy_sell_signal = 'Hold'\n",
    "        \n",
    "    if latest_rsi < 30:\n",
    "        analysis_signal = 'Buy'\n",
    "    elif latest_rsi > 70:\n",
    "        analysis_signal = 'Sell'\n",
    "    else:\n",
    "        analysis_signal = 'Hold'\n",
    "        \n",
    "    if rec_consensus >= 4.00:\n",
    "        analyst_signal = 'Buy'\n",
    "    elif rec_consensus <= 2.00:\n",
    "        analyst_signal = 'Sell'\n",
    "    else:\n",
    "        analyst_signal = 'Hold'\n",
    "    \n",
    "    profit_margins = df.loc[df['ticker_symbol'] == ticker_symbol, ['PM:2021C', 'PM:2022C', 'PM:2023C', 'PM LF']].values[0]\n",
    "    momentum = (profit_margins[-1] - profit_margins[-2]) / profit_margins[-2]\n",
    "    profit_margin_series = pd.Series(profit_margins, index=['2021', '2022', '2023', 'LF'])\n",
    "    # Calculate MACD\n",
    "    pm_short_ema = profit_margin_series.ewm(span=12, adjust=False).mean()\n",
    "    pm_long_ema = profit_margin_series.ewm(span=26, adjust=False).mean()\n",
    "    pm_macd = short_ema - long_ema\n",
    "    pm_signal = macd.ewm(span=9, adjust=False).mean()\n",
    "    pm_macd_diff = macd - signal\n",
    "    growth_rates = []\n",
    "    for i in range(1, len(profit_margins)):\n",
    "        growth_rate = (profit_margins[i] - profit_margins[i-1]) / profit_margins[i-1]\n",
    "        growth_rates.append(growth_rate)\n",
    "\n",
    "    # Calculate the total growth rate by compounding annual growth rates\n",
    "    total_growth_rate = 1\n",
    "    for growth_rate in growth_rates:\n",
    "        total_growth_rate *= (1 + growth_rate)\n",
    "\n",
    "    total_growth_rate -= 1  # Convert to percentage format\n",
    "\n",
    "    # Add the total growth rate to the DataFrame\n",
    "    df.loc[df['ticker_symbol'] == ticker_symbol, 'Total Profit Margin Growth'] = total_growth_rate * 100\n",
    "    df.loc[df['ticker_symbol'] == ticker_symbol, 'Profit Margin Momentum'] = momentum * 100\n",
    "    # Update the main df with calculated values\n",
    "    df.loc[df['ticker_symbol'] == ticker_symbol, 'Log Return'] = latest_log_return\n",
    "    df.loc[df['ticker_symbol'] == ticker_symbol, 'Momentum'] = latest_momentum\n",
    "    df.loc[df['ticker_symbol'] == ticker_symbol, 'Expected Annual Return'] = expected_annual_return\n",
    "    df.loc[df['ticker_symbol'] == ticker_symbol, 'Buy Sell Signal'] = buy_sell_signal\n",
    "    df.loc[df['ticker_symbol'] == ticker_symbol, 'Analysis Signal'] = analysis_signal\n",
    "    df.loc[df['ticker_symbol'] == ticker_symbol, 'Recommendation Signal'] = analyst_signal\n",
    "    df.loc[df['ticker_symbol'] == ticker_symbol, 'PM_MACD'] = pm_macd.iloc[-1]\n",
    "    df.loc[df['ticker_symbol'] == ticker_symbol, 'PM_Signal'] = pm_signal.iloc[-1]\n",
    "    df.loc[df['ticker_symbol'] == ticker_symbol, 'PM_MACD_Diff'] = pm_macd_diff.iloc[-1]\n",
    "    # df.loc[df['ticker_symbol'] == ticker_symbol, 'PM YoY Growth'] = [yoy_growth_rates] \n",
    "    # df.loc[df['ticker_symbol'] == ticker_symbol, 'PM Trend'] = trend\n",
    "    # df.loc[df['ticker_symbol'] == ticker_symbol, 'PM Avg Growth Rate'] = avg_growth_rate\n",
    "    # df.loc[df['ticker_symbol'] == ticker_symbol, 'PM Momentum'] = overall_momentum_sign\n",
    "\n",
    "    return df\n",
    "\n",
    "# Assuming ticker_df is the original DataFrame containing ticker symbols\n",
    "# ticker_df = pd.DataFrame({'ticker_symbol': ['AAPL', 'MSFT', 'GOOGL', 'TSLA']})\n",
    "\n",
    "# Prepare the DataFrame with empty columns for analysis data\n",
    "df_cleaned['Log Return'] = np.nan\n",
    "df_cleaned['Momentum'] = np.nan\n",
    "df_cleaned['Expected Annual Return'] = np.nan\n",
    "df_cleaned['Buy Sell Signal'] = np.nan\n",
    "df_cleaned['Analysis Signal'] = np.nan\n",
    "df_cleaned['Recommendation Signal'] = np.nan\n",
    "df_cleaned['Total Profit Margin Growth'] = np.nan\n",
    "df_cleaned['Profit Margin Momentum'] = np.nan\n",
    "df_cleaned['PM_MACD'] = np.nan\n",
    "df_cleaned['PM_Signal'] = np.nan\n",
    "df_cleaned['PM_MACD_Diff'] = np.nan\n",
    "\n",
    "\n",
    "# Get income statements with profit margin calculations\n",
    "# income_df = get_income_statements(df_cleaned)\n",
    "\n",
    "# Loop over each ticker symbol in df_cleaned and update the main DataFrame\n",
    "for ticker in df_cleaned['ticker_symbol']:\n",
    "    df_cleaned = analyze_ticker_and_update_df(ticker, df_cleaned)\n",
    "\n",
    "# Display the updated df_cleaned DataFrame\n",
    "df_cleaned\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cf13a9c-3c30-4ede-a90e-854f1ab9f3e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the mean of the 'Profit Margin Growth' column for each ticker symbol in combined_df\n",
    "mean_profit_margin_growth = combined_df.groupby('symbol')['Profit Margin Growth'].mean().reset_index()\n",
    "# Rename the 'Profit Margin Growth' column to distinguish it in df_cleaned\n",
    "mean_profit_margin_growth.rename(columns={'Profit Margin Growth': 'Mean Profit Margin Growth'}, inplace=True)\n",
    "# Merge the mean profit margin growth data into df_cleaned based on the ticker symbol\n",
    "df_cleaned = df_cleaned.merge(mean_profit_margin_growth, how='left', left_on='ticker_symbol', right_on='symbol')\n",
    "# Drop the extra 'symbol' column that results from the merge if it's not needed\n",
    "df_cleaned.drop(columns=['symbol'], inplace=True)\n",
    "# Display the updated df_cleaned DataFrame\n",
    "df_cleaned\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9eb567ee-79ee-4315-bb92-ea03433d430e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_cleaned['asset_turnover'] = df_cleaned['Revenue LF'] / df_cleaned['Tot Assets LF']\n",
    "# df_cleaned['financial_leverage'] = df_cleaned['Tot Assets LF'] / df_cleaned['Tot Eqty LF']\n",
    "df_cleaned['earnings_yield'] = 1 / df_cleaned['P/E:D-1']\n",
    "df_cleaned['book_to_market'] = 1 / df_cleaned['P/B:D-1']\n",
    "df_cleaned['CF/P'] = 1 / df_cleaned['P/CF:D-1']\n",
    "df_filtered = df_cleaned[['Ticker', 'Short Name', 'earnings_yield', 'book_to_market', 'CF/P', 'Expected Annual Return', 'Momentum', 'ticker_symbol', 'Log Return', 'Mean Profit Margin Growth', 'GICS Sector', 'GICS Ind Name', 'ROE:Y', 'Recommendation Signal', 'PM_MACD_Diff']].dropna()\n",
    "df_filtered\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82d5d077-f6de-422b-9c40-fba279220825",
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = {\n",
    "    'ROE:Y': 0.15,\n",
    "    'Momentum': 0.25,\n",
    "    'Profit Margin MACD': 0.15,\n",
    "    'earnings_yield': 0.20,\n",
    "    'CF/P': 0.10,\n",
    "    'book_to_market': 0.15\n",
    "}\n",
    "# Z-score the factors by sector\n",
    "df_filtered['Z_ROE'] = df_filtered.groupby('GICS Sector')['ROE:Y'].transform(lambda x: stats.zscore(x, nan_policy='omit'))\n",
    "df_filtered['Z_Momentum'] = df_filtered.groupby('GICS Sector')['Momentum'].transform(lambda x: stats.zscore(x, nan_policy='omit'))\n",
    "df_filtered['Z_Profit Margin MACD'] = df_filtered.groupby('GICS Sector')['PM_MACD_Diff'].transform(lambda x: stats.zscore(x, nan_policy='omit'))\n",
    "df_filtered['Z_earnings_yield'] = df_filtered.groupby('GICS Sector')['earnings_yield'].transform(lambda x: stats.zscore(x, nan_policy='omit'))\n",
    "df_filtered['Z_CF/P'] = df_filtered.groupby('GICS Sector')['CF/P'].transform(lambda x: stats.zscore(x, nan_policy='omit'))\n",
    "df_filtered['Z_book_to_market'] = df_filtered.groupby('GICS Sector')['book_to_market'].transform(lambda x: stats.zscore(x, nan_policy='omit'))\n",
    "# Compute a weighted score for each company\n",
    "df_filtered['Weighted_Score'] = (df_filtered['Z_ROE'] * weights['ROE:Y'] +\n",
    "                                 df_filtered['Z_Momentum'] * weights['Momentum'] +\n",
    "                                df_filtered['Z_Profit Margin MACD'] * weights['Profit Margin MACD'] +\n",
    "                                df_filtered['Z_earnings_yield'] * weights['earnings_yield'] + df_filtered['Z_CF/P'] * weights['CF/P'] +\n",
    "                                df_filtered['Z_book_to_market'] * weights['book_to_market'])\n",
    "\n",
    "# Sort by the weighted score\n",
    "df_sorted = df_filtered.sort_values(by='Weighted_Score', ascending=False)\n",
    "# Extract the top 40 and bottom 40 companies\n",
    "top_60_long = df_sorted.head(100)\n",
    "top_60_long.to_excel(\"top_100_long_companies_november_14_UPDATE.xlsx\", index=False)\n",
    "# If you want to display the results in the console, you can print them as well:\n",
    "# print(\"Top 40 Companies:\")\n",
    "# print(top_40_long)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "273f0d73-e469-484f-a8c7-d705671558ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = {\n",
    "    'ROE:Y': 0.05,\n",
    "    'Momentum': 0.50,\n",
    "    'Profit Margin MACD': 0.05,\n",
    "    'earnings_yield': 0.10,\n",
    "    'CF/P': 0.15,\n",
    "    'book_to_market': 0.15\n",
    "}\n",
    "# Z-score the factors by sector\n",
    "df_filtered['Z_ROE'] = df_filtered.groupby('GICS Sector')['ROE:Y'].transform(lambda x: stats.zscore(x, nan_policy='omit'))\n",
    "df_filtered['Z_Momentum'] = df_filtered.groupby('GICS Sector')['Momentum'].transform(lambda x: stats.zscore(x, nan_policy='omit'))\n",
    "df_filtered['Z_Profit Margin MACD'] = df_filtered.groupby('GICS Sector')['Mean Profit Margin Growth'].transform(lambda x: stats.zscore(x, nan_policy='omit'))\n",
    "df_filtered['Z_earnings_yield'] = df_filtered.groupby('GICS Sector')['earnings_yield'].transform(lambda x: stats.zscore(x, nan_policy='omit'))\n",
    "df_filtered['Z_CF/P'] = df_filtered.groupby('GICS Sector')['CF/P'].transform(lambda x: stats.zscore(x, nan_policy='omit'))\n",
    "df_filtered['Z_book_to_market'] = df_filtered.groupby('GICS Sector')['book_to_market'].transform(lambda x: stats.zscore(x, nan_policy='omit'))\n",
    "# Compute a weighted score for each company\n",
    "df_filtered['Weighted_Score'] = (df_filtered['Z_ROE'] * weights['ROE:Y'] +\n",
    "                                 df_filtered['Z_Momentum'] * weights['Momentum'] + df_filtered['Z_Profit Margin MACD'] * weights['Profit Margin MACD'] +\n",
    "                                df_filtered['Z_earnings_yield'] * weights['earnings_yield'] + df_filtered['Z_CF/P'] * weights['CF/P'] +\n",
    "                                df_filtered['Z_book_to_market'] * weights['book_to_market'])\n",
    "\n",
    "# Sort by the weighted score\n",
    "df_sorted = df_filtered.sort_values(by='Weighted_Score', ascending=False).dropna(axis=0)\n",
    "\n",
    "# Extract the top 40 and bottom 40 companies\n",
    "bottom_60_short = df_sorted.tail(100)\n",
    "bottom_60_short.to_excel(\"bottom_100_short_companies_november_14_UPDATED.xlsx\", index=False)\n",
    "\n",
    "# print(\"\\nBottom 40 Companies:\")\n",
    "# print(bottom_40_short)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f89e82a0-a516-4744-8e53-9745f854ff47",
   "metadata": {},
   "outputs": [],
   "source": [
    "top_40_long.to_excel(\"/Downloads/top_40_companies_latest.xlsx\", index=False)\n",
    "bottom_40_short.to_excel(\"/Downloads/bottom_40_companies_latest.xlsx\", index=False)\n",
    "# Display download links\n",
    "display(FileLink(\"/Downloads/top_40_companies_latest.xlsx\"))\n",
    "display(FileLink(\"/Downloads/bottom_40_companies_latest.xlsx\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb5ce0a1-9680-45e8-8705-1a937a9328dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "current_directory = os.getcwd()\n",
    "print(\"Current working directory:\", current_directory)\n",
    "\n",
    "# Save files directly to the current directory\n",
    "top_40_long.to_excel(\"top_40_companies_long_buy_latest.xlsx\", index=False)\n",
    "bottom_40_short.to_excel(\"bottom_40_companies_short_sell_latest.xlsx\", index=False)\n",
    "\n",
    "print(\"Files saved to:\", current_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d1edde2-7706-418f-8c6f-56826f0f2e52",
   "metadata": {},
   "outputs": [],
   "source": [
    "updated_file_path = 'Financial Data Historical.csv'\n",
    "df_cleaned = pd.read_csv(updated_file_path)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03ff85a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sorted = df_filtered.sort_values(by='Weighted_Score', ascending=False)\n",
    "top_60_long = df_sorted.head(60)\n",
    "bottom_60_short = df_sorted.tail(60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5547119e",
   "metadata": {},
   "outputs": [],
   "source": [
    "top_60_long"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8040d304",
   "metadata": {},
   "outputs": [],
   "source": [
    "top_60_long_symbols = top_60_long['ticker_symbol'].tolist()\n",
    "bottom_60_short_symbols = bottom_60_short['ticker_symbol'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da6ff148",
   "metadata": {},
   "outputs": [],
   "source": [
    "bottom_60_short_symbols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b772806f",
   "metadata": {},
   "outputs": [],
   "source": [
    "top_60_long_data = Ticker(top_60_long_symbols).history(start='2022-11-06', end='2024-11-05').dropna()\n",
    "bottom_60_short_data = Ticker(bottom_60_short_symbols).history(start='2022-11-06', end='2024-11-05').dropna()\n",
    "market_data = Ticker('^GSPC').history(start='2022-11-06', end='2024-11-05').dropna()\n",
    "\n",
    "market_data['log_return'] = np.log(market_data['adjclose'] / market_data['adjclose'].shift(1))\n",
    "top_60_long_data['log_return'] = np.log(top_60_long_data['adjclose'] / top_60_long_data['adjclose'].shift(1))\n",
    "bottom_60_short_data['log_return'] = np.log(bottom_60_short_data['adjclose'] / bottom_60_short_data['adjclose'].shift(1))\n",
    "\n",
    "bottom_60_short_data['std_dev_log_return'] = bottom_60_short_data.groupby(level='symbol')['log_return'].transform('std')\n",
    "top_60_long_data['std_dev_log_return'] = top_60_long_data.groupby(level='symbol')['log_return'].transform('std')\n",
    "market_data['std_dev_log_return'] = market_data['log_return'].std()\n",
    "\n",
    "market_data['variance'] = market_data['log_return'].var()\n",
    "market_data['expected_annual_return'] = market_data['log_return'].mean() * 252\n",
    "market_data['expected_annual_volatility'] = market_data['std_dev_log_return'].mean() * sqrt(252)\n",
    "market_variance = market_data['log_return'].var()\n",
    "market_return = market_data['log_return'].mean()\n",
    "\n",
    "market_data = market_data.dropna()\n",
    "top_60_long_data = top_60_long_data.dropna()\n",
    "bottom_60_short_data = bottom_60_short_data.dropna()\n",
    "\n",
    "bottom_60_short_data['std_dev_log_return'] = bottom_60_short_data.groupby(level='symbol')['log_return'].transform('std')\n",
    "top_60_long_data['std_dev_log_return'] = top_60_long_data.groupby(level='symbol')['log_return'].transform('std')\n",
    "market_data['std_dev_log_return'] = market_data['log_return'].std()\n",
    "\n",
    "r_f = risk_free_rate / 252 \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "504751e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_cov_with_market(stock_returns):\n",
    "    # Align both series by date to ensure they match for covariance calculation\n",
    "    stock_returns_aligned, market_returns_aligned = stock_returns.align(aligned_market_returns, join='inner')\n",
    "    return stock_returns_aligned.cov(market_returns_aligned)\n",
    "\n",
    "\n",
    "def calculate_cov_with_market_long(stock_returns):\n",
    "    # Align both series by date to ensure they match for covariance calculation\n",
    "    stock_returns_aligned, market_returns_aligned = stock_returns.align(aligned_market_returns, join='inner')\n",
    "    return stock_returns_aligned.cov(market_returns_aligned)\n",
    "\n",
    "def garch_volatility_forecast(returns, periods=1):\n",
    "    # Fit GARCH(1,1) model to the stock's log returns\n",
    "    model = arch_model(returns, vol='Garch', p=1, q=1, dist='normal')\n",
    "    model_fit = model.fit(disp=\"off\")\n",
    "    # Forecast volatility for the next `periods` period(s)\n",
    "    forecasts = model_fit.forecast(horizon=periods)\n",
    "    # Extract forecasted conditional volatility (standard deviation)\n",
    "    return forecasts.variance.iloc[-1, 0] ** 0.5  # Taking the square root for volatility\n",
    "\n",
    "short_dataset_indexed = bottom_60_short_data.reset_index()\n",
    "long_dataset_indexed = top_60_long_data.reset_index()\n",
    "market_dataset_indexed = market_data.reset_index()\n",
    "\n",
    "# Set 'symbol' as the new index\n",
    "short_dataset = short_dataset_indexed.set_index('date')\n",
    "long_dataset = long_dataset_indexed.set_index('date')\n",
    "market_dataset = market_dataset_indexed.set_index('date')\n",
    "\n",
    "aligned_market_returns = market_dataset['log_return'].reindex(short_dataset.index.get_level_values('date')).dropna()\n",
    "aligned_market_returns_long = market_dataset['log_return'].reindex(long_dataset.index.get_level_values('date')).dropna()\n",
    "short_dataset['cov_with_market'] = short_dataset.groupby('symbol')['log_return'].transform(calculate_cov_with_market)\n",
    "long_dataset['cov_with_market'] = long_dataset.groupby('symbol')['log_return'].transform(calculate_cov_with_market_long)\n",
    "short_dataset['stock_beta'] = (short_dataset['cov_with_market']/market_variance)\n",
    "long_dataset['stock_beta'] = (long_dataset['cov_with_market']/market_variance)\n",
    "long_dataset['expected_return_capm'] = r_f + long_dataset['stock_beta'] * (market_return - r_f)\n",
    "short_dataset['expected_return_capm'] = r_f + short_dataset['stock_beta'] * (market_return - r_f)\n",
    "\n",
    "short_dataset['garch_volatility'] = short_dataset.groupby('symbol')['log_return'].transform(garch_volatility_forecast)\n",
    "long_dataset['garch_volatility'] = long_dataset.groupby('symbol')['log_return'].transform(garch_volatility_forecast)\n",
    "\n",
    "# Adjust expected return using forecasted volatility in a CAPM-like formula\n",
    "short_dataset['expected_return_garch'] = r_f + short_dataset['stock_beta'] * (market_return - r_f) * short_dataset['garch_volatility']\n",
    "long_dataset['expected_return_garch'] = r_f + long_dataset['stock_beta'] * (market_return - r_f) * long_dataset['garch_volatility']\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86169708",
   "metadata": {},
   "outputs": [],
   "source": [
    "short_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e4c8fba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Engineering for the Long Portfolio (top_60_long_data)\n",
    "long_features = long_dataset[['log_return', 'std_dev_log_return', 'stock_beta']]  # Example features\n",
    "X_long = long_features.dropna()\n",
    "y_long = X_long['log_return']  # Target: Expected Return (log_return)\n",
    "\n",
    "# Split the data into training and testing sets for the Long Portfolio\n",
    "X_train_long, X_test_long, y_train_long, y_test_long = train_test_split(X_long, y_long, test_size=0.2, random_state=42)\n",
    "\n",
    "# 1. Random Forest for the Long Portfolio\n",
    "rf_model_long = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "rf_model_long.fit(X_train_long, y_train_long)\n",
    "\n",
    "# Predict for Long Portfolio\n",
    "y_pred_rf_long = rf_model_long.predict(X_test_long)\n",
    "mse_rf_long = mean_squared_error(y_test_long, y_pred_rf_long)\n",
    "print(f\"Random Forest MSE for Long Portfolio: {mse_rf_long}\")\n",
    "\n",
    "# 2. Gradient Boosting for the Long Portfolio\n",
    "gb_model_long = GradientBoostingRegressor(n_estimators=100, random_state=42)\n",
    "gb_model_long.fit(X_train_long, y_train_long)\n",
    "\n",
    "# Predict for Long Portfolio\n",
    "y_pred_gb_long = gb_model_long.predict(X_test_long)\n",
    "mse_gb_long = mean_squared_error(y_test_long, y_pred_gb_long)\n",
    "print(f\"Gradient Boosting MSE for Long Portfolio: {mse_gb_long}\")\n",
    "\n",
    "# 3. Support Vector Machine (SVM) for the Long Portfolio\n",
    "svm_model_long = SVR(kernel='rbf', C=1.0, epsilon=0.1)\n",
    "svm_model_long.fit(X_train_long, y_train_long)\n",
    "\n",
    "# Predict for Long Portfolio\n",
    "y_pred_svm_long = svm_model_long.predict(X_test_long)\n",
    "mse_svm_long = mean_squared_error(y_test_long, y_pred_svm_long)\n",
    "print(f\"SVM MSE for Long Portfolio: {mse_svm_long}\")\n",
    "\n",
    "# Store predicted returns for the Long Portfolio\n",
    "long_dataset['predicted_return_rf'] = rf_model_long.predict(long_features.dropna())\n",
    "long_dataset['predicted_return_gb'] = gb_model_long.predict(long_features.dropna())\n",
    "long_dataset['predicted_return_svm'] = svm_model_long.predict(long_features.dropna())\n",
    "\n",
    "# Repeat the process for the Short Portfolio (bottom_60_short_data)\n",
    "\n",
    "# Feature Engineering for the Short Portfolio (bottom_60_short_data)\n",
    "short_features = short_dataset[['log_return', 'std_dev_log_return', 'stock_beta']]  # Example features\n",
    "X_short = short_features.dropna()\n",
    "y_short = X_short['log_return']  # Target: Expected Return (log_return)\n",
    "\n",
    "# Split the data into training and testing sets for the Short Portfolio\n",
    "X_train_short, X_test_short, y_train_short, y_test_short = train_test_split(X_short, y_short, test_size=0.2, random_state=42)\n",
    "\n",
    "# 1. Random Forest for the Short Portfolio\n",
    "rf_model_short = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "rf_model_short.fit(X_train_short, y_train_short)\n",
    "\n",
    "# Predict for Short Portfolio\n",
    "y_pred_rf_short = rf_model_short.predict(X_test_short)\n",
    "mse_rf_short = mean_squared_error(y_test_short, y_pred_rf_short)\n",
    "print(f\"Random Forest MSE for Short Portfolio: {mse_rf_short}\")\n",
    "\n",
    "# 2. Gradient Boosting for the Short Portfolio\n",
    "gb_model_short = GradientBoostingRegressor(n_estimators=100, random_state=42)\n",
    "gb_model_short.fit(X_train_short, y_train_short)\n",
    "\n",
    "# Predict for Short Portfolio\n",
    "y_pred_gb_short = gb_model_short.predict(X_test_short)\n",
    "mse_gb_short = mean_squared_error(y_test_short, y_pred_gb_short)\n",
    "print(f\"Gradient Boosting MSE for Short Portfolio: {mse_gb_short}\")\n",
    "\n",
    "# 3. Support Vector Machine (SVM) for the Short Portfolio\n",
    "svm_model_short = SVR(kernel='rbf', C=1.0, epsilon=0.1)\n",
    "svm_model_short.fit(X_train_short, y_train_short)\n",
    "\n",
    "# Predict for Short Portfolio\n",
    "y_pred_svm_short = svm_model_short.predict(X_test_short)\n",
    "mse_svm_short = mean_squared_error(y_test_short, y_pred_svm_short)\n",
    "print(f\"SVM MSE for Short Portfolio: {mse_svm_short}\")\n",
    "\n",
    "# Store predicted returns for the Short Portfolio\n",
    "short_dataset['predicted_return_rf'] = rf_model_short.predict(short_features.dropna())\n",
    "short_dataset['predicted_return_gb'] = gb_model_short.predict(short_features.dropna())\n",
    "short_dataset['predicted_return_svm'] = svm_model_short.predict(short_features.dropna())\n",
    "\n",
    "# Now both the long_dataset and short_dataset contain the predicted returns using the three models\n",
    "short_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70dbd7dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_single_index_short['stock_beta'] = (df_single_index_short['cov_with_market']/test_variance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "562c7a02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pivot the dataframe to have each 'symbol' as a column, with 'log_return' as the values\n",
    "returns_pivot = df_single_index_short.pivot(columns='symbol', values='log_return')\n",
    "\n",
    "# Drop any rows with NaN values in 'log_return' columns\n",
    "returns_pivot = returns_pivot.dropna()\n",
    "\n",
    "# Calculate the covariance matrix\n",
    "cov_matrix = returns_pivot.cov()\n",
    "\n",
    "cov_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e661f4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "weight_count = len(bottom_60_short_symbols)\n",
    "weight_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb6df52c",
   "metadata": {},
   "outputs": [],
   "source": [
    "short_weights = np.zeros((weight_count,),dtype = float )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "898fdf29",
   "metadata": {},
   "outputs": [],
   "source": [
    "expected_returns = np.array([])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8bcbbd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "portfolio_1_tickers = ['AAPL', 'NFLX']\n",
    "portfolio_2_tickers = ['AAPL', 'WBD', '^GSPC', '^IXIC', 'NFLX', 'DIS']\n",
    "portfolio_1_data = Ticker(portfolio_1_tickers).history(period='1d', start='2019-03-25', end='2024-09-30').dropna()\n",
    "portfolio_2_data = Ticker(portfolio_2_tickers).history(period='1d', start='2019-03-25', end='2024-09-30').dropna()\n",
    "\n",
    "portfolio_1_asset_count = len(portfolio_1_tickers)\n",
    "portfolio_2_asset_count = len(portfolio_2_tickers)\n",
    "\n",
    "portfolio_1_weights = np.random.random(portfolio_1_asset_count)\n",
    "portfolio_2_weights = np.random.random(portfolio_2_asset_count)\n",
    "\n",
    "portfolio_1_weights /= np.sum(portfolio_1_weights)\n",
    "portfolio_2_weights /= np.sum(portfolio_2_weights)\n",
    "\n",
    "portfolio_1_closing_prices = pd.DataFrame(portfolio_1_data['adjclose'])\n",
    "portfolio_2_closing_prices = pd.DataFrame(portfolio_2_data['adjclose'])\n",
    "\n",
    "portfolio_1_dataframe = portfolio_1_closing_prices.pivot_table(index='date', columns='symbol', values='adjclose')\n",
    "portfolio_2_dataframe = portfolio_2_closing_prices.pivot_table(index='date', columns='symbol', values='adjclose')\n",
    "\n",
    "portfolio_1_log_returns = np.log(portfolio_1_dataframe/portfolio_1_dataframe.shift(1))\n",
    "portfolio_2_log_returns = np.log(portfolio_2_dataframe/portfolio_2_dataframe.shift(1))\n",
    "\n",
    "market_return = portfolio_2_log_returns['^GSPC'].mean() * 252  # Replace with actual market return\n",
    "\n",
    "portfolio_2_log_returns = portfolio_2_log_returns.dropna()\n",
    "portfolio_1_log_returns = portfolio_1_log_returns.dropna()\n",
    "\n",
    "\n",
    "# --- 1. CAPM Expected Return Calculation ---\n",
    "def calculate_capm_return(asset_returns, market_returns):\n",
    "    betas = []\n",
    "    for asset in asset_returns.columns:\n",
    "        # Linear regression to find beta\n",
    "        cov_with_market = np.cov(asset_returns[asset], market_returns)[0, 1]\n",
    "        beta = cov_with_market / np.var(market_returns)\n",
    "        betas.append(beta)\n",
    "    # Calculate expected returns for each asset using CAPM\n",
    "    capm_expected_returns = [risk_free_rate + beta * (market_return - risk_free_rate) for beta in betas]\n",
    "    return capm_expected_returns\n",
    "\n",
    "# Apply CAPM to each portfolio\n",
    "portfolio_1_capm_returns = calculate_capm_return(portfolio_1_log_returns, portfolio_2_log_returns['^GSPC'])\n",
    "portfolio_2_capm_returns = calculate_capm_return(portfolio_2_log_returns, portfolio_2_log_returns['^GSPC'])\n",
    "\n",
    "\n",
    "# --- 2. GARCH Expected Return Calculation ---\n",
    "def calculate_garch_return(asset_returns):\n",
    "    garch_expected_returns = []\n",
    "    for asset in asset_returns.columns:\n",
    "        model = arch_model(asset_returns[asset].dropna(), vol='Garch', p=1, q=1)\n",
    "        garch_fit = model.fit(disp=\"off\")\n",
    "        # Forecast next period's return and volatility\n",
    "        forecast = garch_fit.forecast(horizon=1)\n",
    "        expected_volatility = forecast.variance.values[-1, :][0]\n",
    "        # Expected return considering volatility\n",
    "        garch_expected_returns.append(expected_volatility)  # Adjust if you have a specific return forecast\n",
    "    return garch_expected_returns\n",
    "\n",
    "# Apply GARCH to each portfolio\n",
    "portfolio_1_garch_returns = calculate_garch_return(portfolio_1_log_returns)\n",
    "portfolio_2_garch_returns = calculate_garch_return(portfolio_2_log_returns)\n",
    "\n",
    "\n",
    "# --- 3. Gradient Boosting Expected Return Calculation ---\n",
    "# Prepare data for gradient boosting model\n",
    "def prepare_data(asset_returns):\n",
    "    X, y = [], []\n",
    "    for i in range(1, len(asset_returns)):\n",
    "        X.append(asset_returns.iloc[i - 1])\n",
    "        y.append(asset_returns.iloc[i])\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "X1, y1 = prepare_data(portfolio_1_log_returns)\n",
    "X2, y2 = prepare_data(portfolio_2_log_returns)\n",
    "\n",
    "# Split data for training and testing\n",
    "X1_train, X1_test, y1_train, y1_test = train_test_split(X1, y1, test_size=0.2, random_state=42)\n",
    "X2_train, X2_test, y2_train, y2_test = train_test_split(X2, y2, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train Gradient Boosting Regressor\n",
    "gb_model_1 = GradientBoostingRegressor()\n",
    "gb_model_1.fit(X1_train, y1_train)\n",
    "\n",
    "gb_model_2 = GradientBoostingRegressor()\n",
    "gb_model_2.fit(X2_train, y2_train)\n",
    "\n",
    "# Gradient Boosting Expected Return Calculation\n",
    "def calculate_gradient_boosting_returns(asset_returns):\n",
    "    gb_expected_returns = []\n",
    "    for asset in asset_returns.columns:\n",
    "        # Prepare data for gradient boosting\n",
    "        X, y = prepare_data(asset_returns[[asset]])  # Keep only one asset at a time\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "        \n",
    "        # Train Gradient Boosting Regressor for this asset\n",
    "        gb_model = GradientBoostingRegressor()\n",
    "        gb_model.fit(X_train, y_train.ravel())  # Ensure y_train is 1-dimensional\n",
    "        \n",
    "        # Predict expected return for the asset\n",
    "        gb_return = gb_model.predict([asset_returns[asset].iloc[-1]])[0]\n",
    "        gb_expected_returns.append(gb_return)\n",
    "    \n",
    "    return gb_expected_returns\n",
    "\n",
    "# Apply Gradient Boosting to each portfolio\n",
    "portfolio_1_gb_returns = calculate_gradient_boosting_returns(portfolio_1_log_returns)\n",
    "portfolio_2_gb_returns = calculate_gradient_boosting_returns(portfolio_2_log_returns)\n",
    "\n",
    "# Use CAPM, GARCH, and GB predicted returns to calculate expected return for portfolios\n",
    "expected_return_portfolio_1 = np.dot(portfolio_1_weights, portfolio_1_capm_returns + portfolio_1_garch_returns + [portfolio_1_gb_return])\n",
    "expected_return_portfolio_2 = np.dot(portfolio_2_weights, portfolio_2_capm_returns + portfolio_2_garch_returns + [portfolio_2_gb_return])\n",
    "\n",
    "print(f\"Expected Return (Portfolio 1): {expected_return_portfolio_1:.2f}\")\n",
    "print(f\"Expected Return (Portfolio 2): {expected_return_portfolio_2:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e4fa103",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initializing tickers and fetching data\n",
    "portfolio_1_tickers = ['AAPL', 'NFLX']\n",
    "portfolio_2_tickers = ['AAPL', 'WBD', '^GSPC', '^IXIC', 'NFLX', 'DIS']\n",
    "portfolio_1_data = Ticker(top_60_long_symbols).history(period='1d', start='2019-03-25', end='2024-09-30').dropna()\n",
    "portfolio_2_data = Ticker(portfolio_2_tickers).history(period='1d', start='2019-03-25', end='2024-09-30').dropna()\n",
    "\n",
    "# Setting up portfolio weights\n",
    "portfolio_1_asset_count = len(portfolio_1_tickers)\n",
    "portfolio_2_asset_count = len(portfolio_2_tickers)\n",
    "portfolio_1_weights = np.random.random(portfolio_1_asset_count)\n",
    "portfolio_2_weights = np.random.random(portfolio_2_asset_count)\n",
    "portfolio_1_weights /= np.sum(portfolio_1_weights)\n",
    "portfolio_2_weights /= np.sum(portfolio_2_weights)\n",
    "\n",
    "# Formatting the data\n",
    "portfolio_1_closing_prices = pd.DataFrame(portfolio_1_data['adjclose'])\n",
    "portfolio_2_closing_prices = pd.DataFrame(portfolio_2_data['adjclose'])\n",
    "portfolio_1_dataframe = portfolio_1_closing_prices.pivot_table(index='date', columns='symbol', values='adjclose')\n",
    "portfolio_2_dataframe = portfolio_2_closing_prices.pivot_table(index='date', columns='symbol', values='adjclose')\n",
    "\n",
    "# Log returns\n",
    "portfolio_1_log_returns = np.log(portfolio_1_dataframe / portfolio_1_dataframe.shift(1)).dropna()\n",
    "portfolio_2_log_returns = np.log(portfolio_2_dataframe / portfolio_2_dataframe.shift(1)).dropna()\n",
    "\n",
    "market_return = market_data['log_return'].mean() * 252  # Replace with actual market return\n",
    "\n",
    "# --- 1. CAPM Expected Return Calculation ---\n",
    "def calculate_capm_return(asset_returns, market_returns, risk_free_rate=0.02):\n",
    "    betas = []\n",
    "    for asset in asset_returns.columns:\n",
    "        cov_with_market = np.cov(asset_returns[asset], market_returns)[0, 1]\n",
    "        beta = cov_with_market / np.var(market_returns)\n",
    "        betas.append(beta)\n",
    "    capm_expected_returns = [risk_free_rate + beta * (market_return - risk_free_rate) for beta in betas]\n",
    "    return capm_expected_returns\n",
    "\n",
    "# Apply CAPM to each portfolio\n",
    "portfolio_1_capm_returns = calculate_capm_return(portfolio_1_log_returns, market_data['log_return'])\n",
    "portfolio_2_capm_returns = calculate_capm_return(portfolio_2_log_returns, market_data['log_return'])\n",
    "\n",
    "# --- 2. GARCH Expected Return Calculation ---\n",
    "def calculate_garch_return(asset_returns):\n",
    "    garch_expected_returns = []\n",
    "    for asset in asset_returns.columns:\n",
    "        model = arch_model(asset_returns[asset].dropna(), vol='Garch', p=1, q=1)\n",
    "        garch_fit = model.fit(disp=\"off\")\n",
    "        forecast = garch_fit.forecast(horizon=1)\n",
    "        expected_volatility = forecast.variance.values[-1, :][0]\n",
    "        garch_expected_returns.append(expected_volatility)\n",
    "    return garch_expected_returns\n",
    "\n",
    "# Apply GARCH to each portfolio\n",
    "portfolio_1_garch_returns = calculate_garch_return(portfolio_1_log_returns)\n",
    "portfolio_2_garch_returns = calculate_garch_return(portfolio_2_log_returns)\n",
    "\n",
    "\n",
    "# --- Combine CAPM, GARCH, and Gradient Boosting returns for final expected return ---\n",
    "expected_return_portfolio_1 = np.dot(portfolio_1_weights, np.array(portfolio_1_capm_returns) + np.array(portfolio_1_garch_returns))\n",
    "expected_return_portfolio_2 = np.dot(portfolio_2_weights, np.array(portfolio_2_capm_returns) + np.array(portfolio_2_garch_returns))\n",
    "\n",
    "print(f\"Expected Return (Portfolio 1): {expected_return_portfolio_1:.2f}\")\n",
    "print(f\"Expected Return (Portfolio 2): {expected_return_portfolio_2:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f93b941",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10 (tensorflow)",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
